import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F   
import torch.distributions as distributions 
import numpy as np

from scipy.spatial.distance import cosine
from scipy.stats import gaussian_kde
from scipy.integrate import quad

def get_deepfeat(args, model, img):
    model_name = args.model
    extracted_feat = []
    batch_size = args.batch_size

    class FeatureHook:
        def __init__(self):
            self.features = None
        
        def __call__(self, module, input, output):
            self.features = output.detach().clone()

    hook_fn = FeatureHook()

    # æ ¹æ®æ¨¡å‹ç±»å‹æ³¨å†Œä¸åŒçš„hook
    model_type = model_name['type'] if isinstance(model_name, dict) else model_name
    
    # if 'resnet' in model_type:
    #     hook_handle = model.avgpool.register_forward_hook(hook_fn)
    # elif 'wideresnet' in model_type:
    #     hook_handle = model.avgpool.register_forward_hook(hook_fn)
    # elif 'preactresnet' in model_type:
    #     hook_handle = model.avgpool.register_forward_hook(hook_fn)
    # elif 'vgg' in model_type:
    #     hook_handle = model.classifier[-1].register_forward_hook(hook_fn)
    # elif 'inception' in model_type:
    #     hook_handle = model.avgpool.register_forward_hook(hook_fn)
    # elif 'efficient' in model_type:
    #     hook_handle = model.classifier.register_forward_hook(hook_fn)
    # elif 'googlenet' in model_type:
    #     hook_handle = model.avgpool.register_forward_hook(hook_fn)
    # elif 'mobile' in model_type:
    #     hook_handle = model.classifier[-1].register_forward_hook(hook_fn)
    # elif 'shufflenet' in model_type:
    #     hook_handle = model.avgpool.register_forward_hook(hook_fn)
    # elif 'resnext' in model_type:
    #     hook_handle = model.avgpool.register_forward_hook(hook_fn)
    # else:
    #     # é»˜è®¤æƒ…å†µï¼Œå°è¯•ä½¿ç”¨avgpool
    #     hook_handle = model.avgpool.register_forward_hook(hook_fn)

    if 'resnet' in model_type:
        hook_handle = model.module.avgpool.register_forward_hook(hook_fn)
    elif 'wideresnet' in model_type:
        hook_handle = model.module.avgpool.register_forward_hook(hook_fn)
    elif 'preactresnet' in model_type:
        hook_handle = model.module.avgpool.register_forward_hook(hook_fn)
    elif 'vgg' in model_type:
        hook_handle = model.module.classifier[-1].register_forward_hook(hook_fn)
    elif 'inception' in model_type:
        hook_handle = model.module.avgpool.register_forward_hook(hook_fn)
    elif 'efficient' in model_type:
        hook_handle = model.module.classifier.register_forward_hook(hook_fn)
    elif 'googlenet' in model_type:
        hook_handle = model.module.avgpool.register_forward_hook(hook_fn)
    elif 'mobile' in model_type:
        hook_handle = model.module.classifier[-1].register_forward_hook(hook_fn)
    elif 'shufflenet' in model_type:
        hook_handle = model.module.avgpool.register_forward_hook(hook_fn)
    elif 'resnext' in model_type:
        hook_handle = model.module.avgpool.register_forward_hook(hook_fn)
    else:
        # é»˜è®¤æƒ…å†µï¼Œå°è¯•ä½¿ç”¨avgpool
        hook_handle = model.module.avgpool.register_forward_hook(hook_fn)
    
    imgs = torch.split(img, batch_size, dim=0)
    feat = []
    out = []
    for x in imgs:
        with torch.no_grad():
            y = model(x)
            y = F.softmax(y, dim=1)
        out.append(y.view(x.size(0),-1))
        extracted_feat = hook_fn.features
        feat.append(extracted_feat.squeeze())

        del y, extracted_feat

    feat = torch.cat(feat, dim=0)
    out = torch.cat(out, dim=0)
    hook_handle.remove()

    return feat, out

def get_clsprob(model_name, model, img):
    extracted_feat = []
    def hook_fn(model, input, output):
        nonlocal extracted_feat
        extracted_feat = output.detach()

    if 'resnet18' in model_name['type']:
        hook_handle = model.fc.register_forward_hook(hook_fn)
    
    with torch.no_grad():
        model(img)
        
    feat = extracted_feat.view(extracted_feat.size(0),-1)
    hook_handle.remove()

    return feat

import numpy as np

def compute_cost_matrix(X_s, X_t, metric='sqeuclidean'):
    """
    è®¡ç®—æºç‚¹å’Œç›®æ ‡ç‚¹ä¹‹é—´çš„æˆæœ¬çŸ©é˜µ Mã€‚
    M_ij = cost(X_s_i, X_t_j)

    å‚æ•°:
        X_s (np.ndarray): æºå‘é‡é›†åˆ, å½¢çŠ¶ (n, d)
        X_t (np.ndarray): ç›®æ ‡å‘é‡é›†åˆ, å½¢çŠ¶ (m, d)
        metric (str): ä½¿ç”¨çš„è·ç¦»åº¦é‡, 'sqeuclidean' (å¹³æ–¹æ¬§æ°è·ç¦») æˆ– 'euclidean'

    è¿”å›:
        np.ndarray: æˆæœ¬çŸ©é˜µ, å½¢çŠ¶ (n, m)
    """
    # print("ğŸ“ è®¡ç®—æˆæœ¬çŸ©é˜µ...")
    n_source = X_s.shape[0]
    n_target = X_t.shape[0]
    M = np.zeros((n_source, n_target))

    if metric == 'sqeuclidean':
        # æ›´é«˜æ•ˆçš„å¹¿æ’­è®¡ç®—å¹³æ–¹æ¬§æ°è·ç¦»
        sum_sq_X_s = np.sum(X_s**2, axis=1, keepdims=True)
        sum_sq_X_t = np.sum(X_t**2, axis=1, keepdims=True)
        M = sum_sq_X_s + sum_sq_X_t.T - 2 * X_s @ X_t.T
        M = np.maximum(M, 0) # ç¡®ä¿éè´Ÿï¼Œå¤„ç†æ•°å€¼è¯¯å·®
        # ä½¿ç”¨å¾ªç¯ä»¥ä¾¿ä¸ä¹‹å‰çš„ç‰ˆæœ¬è¡Œä¸ºä¸€è‡´ï¼Œè™½ç„¶æ•ˆç‡ç¨ä½
        # for i in range(n_source):
        #     for j in range(n_target):
        #         M[i, j] = np.sum((X_s[i, :] - X_t[j, :])**2)
    elif metric == 'euclidean':
        for i in range(n_source):
            for j in range(n_target):
                M[i, j] = np.sqrt(np.sum((X_s[i, :] - X_t[j, :])**2))
    else:
        raise ValueError(f"æœªçŸ¥çš„åº¦é‡: {metric}")
    
    # print("æˆæœ¬çŸ©é˜µ M (éƒ¨åˆ†):\n", M[:min(3, n_source), :min(3, n_target)])
    return M

def estimate_point_distribution(points, bandwidth_factor=0.5, min_sigma=1e-3):
    """
    æ ¹æ®ç‚¹é›†è‡ªèº«ä¼°è®¡æ¯ä¸ªç‚¹çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆæƒé‡ï¼‰ã€‚
    æƒé‡è¾ƒå¤§çš„ç‚¹é€šå¸¸æ˜¯é‚£äº›å‘¨å›´æœ‰è¾ƒå¤šå…¶ä»–ç‚¹çš„ç‚¹ï¼ˆå³é«˜å¯†åº¦åŒºåŸŸçš„ç‚¹ï¼‰ã€‚
    ä½¿ç”¨é«˜æ–¯æ ¸çš„å’Œæ¥ä¼°è®¡æ¯ä¸ªç‚¹çš„â€œå¯†åº¦â€ï¼Œç„¶åå½’ä¸€åŒ–ã€‚

    å‚æ•°:
        points (np.ndarray): ç‚¹é›†, å½¢çŠ¶ (N, d)
        bandwidth_factor (float): ç”¨äºç¡®å®šé«˜æ–¯æ ¸å¸¦å®½çš„å› å­ã€‚
                                  å¸¦å®½ sigma = median_pairwise_distance * bandwidth_factor.
        min_sigma (float): å¸¦å®½çš„æœ€å°å€¼ï¼Œé˜²æ­¢ sigma è¿‡å°æˆ–ä¸º0ã€‚

    è¿”å›:
        np.ndarray: æ¯ä¸ªç‚¹çš„æƒé‡, å½¢çŠ¶ (N,), æ€»å’Œä¸º1ã€‚
    """
    # print(f"ğŸ“Š ä¼°ç®— {points.shape[0]} ä¸ªç‚¹çš„åˆ†å¸ƒ (æ–¹æ³•: é«˜æ–¯æ ¸å¯†åº¦å’Œ)...")
    N = points.shape[0]
    if N == 0:
        return np.array([])
    if N == 1:
        return np.array([1.0])

    # 1. è®¡ç®—ç‚¹å¯¹ä¹‹é—´çš„å¹³æ–¹æ¬§æ°è·ç¦»
    # (x-y)^2 = x^2 - 2xy + y^2
    points_sq_sum = np.sum(points**2, axis=1) # æ¯ä¸ªç‚¹å„ç»´åº¦å¹³æ–¹å’Œ
    # dist_sq_matrix[i,j] = ||points[i] - points[j]||^2
    dist_sq_matrix = points_sq_sum[:, np.newaxis] + points_sq_sum[np.newaxis, :] - 2 * (points @ points.T)
    dist_sq_matrix = np.maximum(dist_sq_matrix, 0) # ç¡®ä¿éè´Ÿï¼Œå¤„ç†æ•°å€¼è¯¯å·®

    # 2. ç¡®å®šå¸¦å®½ sigma
    sigma = min_sigma # é»˜è®¤æœ€å°å¸¦å®½
    if N > 1:
        pairwise_distances = np.sqrt(dist_sq_matrix[np.triu_indices(N, k=1)]) # æå–ä¸Šä¸‰è§’éƒ¨åˆ†çš„è·ç¦»ï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰
        if len(pairwise_distances) > 0:
            median_dist = np.median(pairwise_distances)
            # å¦‚æœæ‰€æœ‰ç‚¹é‡åˆï¼Œmedian_dist å¯èƒ½ä¸º0
            calculated_sigma = median_dist * bandwidth_factor
            sigma = max(calculated_sigma, min_sigma)
        # å¦‚æœåªæœ‰ä¸¤ä¸ªç‚¹ä¸”å®ƒä»¬é‡åˆ (pairwise_distancesä¸ºç©ºæˆ–å…¨0), sigma ä¿æŒ min_sigma
        
    # print(f"ç”¨äºåˆ†å¸ƒä¼°è®¡çš„é«˜æ–¯æ ¸å¸¦å®½ sigma: {sigma:.4f}")

    # 3. è®¡ç®—æ¯ä¸ªç‚¹çš„â€œå¯†åº¦â€æˆ–â€œå½±å“åŠ›â€
    # density_i = sum_j exp(-||points_i - points_j||^2 / (2 * sigma^2))
    # å¯¹è§’çº¿ä¸Šçš„ dist_sq_matrix[i,i] æ˜¯ 0ï¼Œæ‰€ä»¥ exp(0) = 1ã€‚
    # è¿™æ„å‘³ç€æ¯ä¸ªç‚¹å¯¹è‡ªèº«çš„è´¡çŒ®æ˜¯1ã€‚
    kernel_matrix = np.exp(-dist_sq_matrix / (2 * sigma**2 + 1e-10))
    raw_weights = np.sum(kernel_matrix, axis=1) # æ¯ä¸ªç‚¹æ˜¯å…¶æ‰€åœ¨è¡Œï¼ˆæˆ–åˆ—ï¼‰çš„å’Œ

    # 4. å½’ä¸€åŒ–æƒé‡
    sum_raw_weights = np.sum(raw_weights)
    if sum_raw_weights == 0: # æ‰€æœ‰æƒé‡éƒ½æ˜¯0ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœsigmaéå¸¸å°ä¸”ç‚¹ç›¸è·å¾ˆè¿œï¼‰
        # print("âš ï¸ æ‰€æœ‰åŸå§‹æƒé‡ä¸º0ï¼Œè¿”å›å‡åŒ€åˆ†å¸ƒã€‚")
        return np.ones(N) / N
        
    weights = raw_weights / sum_raw_weights
    
    # print(f"ä¼°ç®—å¾—åˆ°çš„æƒé‡ (å‰å‡ ä¸ª): {weights[:min(5,N)]}")
    return weights

def Sinkhorn_dist(X_s, X_t, a=None, b=None, reg=0.1, num_iters=100, tol=1e-9, verbose=False,
                       estimate_marginals=False, marginals_bandwidth_factor=0.5):
    """
    ä½¿ç”¨ Sinkhorn ç®—æ³•è®¡ç®—ä¸¤ä¸ªå‘é‡é›†åˆä¹‹é—´çš„æœ€ä¼˜ä¼ è¾“è·ç¦»ã€‚

    å‚æ•°:
        X_s (np.ndarray): æºå‘é‡é›†åˆ, å½¢çŠ¶ (n, d)
        X_t (np.ndarray): ç›®æ ‡å‘é‡é›†åˆ, å½¢çŠ¶ (m, d)
        a (np.ndarray, optional): æºåˆ†å¸ƒ (æƒé‡), é•¿åº¦ nã€‚
        b (np.ndarray, optional): ç›®æ ‡åˆ†å¸ƒ (æƒé‡), é•¿åº¦ mã€‚
        reg (float): ç†µæ­£åˆ™åŒ–å‚æ•° (lambda)ã€‚
        num_iters (int): æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚
        tol (float): æ”¶æ•›çš„å®¹å¿åº¦ã€‚
        verbose (bool): æ˜¯å¦æ‰“å°è¿­ä»£è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ã€‚
        estimate_marginals (bool): å¦‚æœä¸ºTrueä¸”aæˆ–bä¸ºNoneï¼Œåˆ™ä»æ•°æ®ä¼°è®¡è¾¹é™…åˆ†å¸ƒã€‚
                                    å¦åˆ™ï¼Œå¦‚æœaæˆ–bä¸ºNoneï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒã€‚
        marginals_bandwidth_factor (float): ç”¨äºä¼°è®¡è¾¹é™…åˆ†å¸ƒæ—¶çš„é«˜æ–¯æ ¸å¸¦å®½å› å­ã€‚

    è¿”å›:
        tuple: (P, dist)
            P (np.ndarray): è¿‘ä¼¼çš„æœ€ä¼˜ä¼ è¾“è®¡åˆ’, å½¢çŠ¶ (n, m)
            dist (float): Sinkhorn è·ç¦» (æœ€ä¼˜ä¼ è¾“æˆæœ¬)
    """
    # print("\nğŸŒ€ å¼€å§‹ Sinkhorn ç®—æ³•è®¡ç®—...")
    n = X_s.shape[0]
    m = X_t.shape[0]

    # 1. å¤„ç†/ä¼°ç®—è¾¹é™…åˆ†å¸ƒ a å’Œ b
    if a is None:
        if estimate_marginals and n > 0:
            # print("æºåˆ†å¸ƒ 'a' æœªæä¾›ï¼Œå°†ä»æºç‚¹é›† X_s ä¼°è®¡...")
            a = estimate_point_distribution(X_s, bandwidth_factor=marginals_bandwidth_factor)
        elif n > 0:
            a = np.ones(n) / n
            # print(f"æºåˆ†å¸ƒ 'a' æœªæä¾›ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ (æ¯ä¸ªå…ƒç´  {1/n:.4f})")
        else:
            a = np.array([]) # ç©ºè¾“å…¥
    
    if b is None:
        if estimate_marginals and m > 0:
            # print("ç›®æ ‡åˆ†å¸ƒ 'b' æœªæä¾›ï¼Œå°†ä»ç›®æ ‡ç‚¹é›† X_t ä¼°è®¡...")
            b = estimate_point_distribution(X_t, bandwidth_factor=marginals_bandwidth_factor)
        elif m > 0:
            b = np.ones(m) / m
            # print(f"ç›®æ ‡åˆ†å¸ƒ 'b' æœªæä¾›ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ (æ¯ä¸ªå…ƒç´  {1/m:.4f})")
        else:
            b = np.array([]) # ç©ºè¾“å…¥

    if n == 0 or m == 0:
        # print("âš ï¸ æºç‚¹é›†æˆ–ç›®æ ‡ç‚¹é›†ä¸ºç©ºï¼Œæ— æ³•è®¡ç®— Sinkhorn è·ç¦»ã€‚")
        return np.array([]).reshape(n,m), 0.0

    if not (np.isclose(np.sum(a), 1.0, atol=1e-6) and np.isclose(np.sum(b), 1.0, atol=1e-6)):
        print(f"âš ï¸ è­¦å‘Š: åˆ†å¸ƒ 'a' (sum={np.sum(a):.6f}) æˆ– 'b' (sum={np.sum(b):.6f}) çš„å’Œä¸æ¥è¿‘ 1ã€‚")
        print("   ç¡®ä¿ä¼°ç®—çš„æˆ–æä¾›çš„åˆ†å¸ƒæ˜¯æœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒã€‚")

    # 2. è®¡ç®—æˆæœ¬çŸ©é˜µ M
    M = compute_cost_matrix(X_s, X_t)

    # 3. è®¡ç®— Kernel çŸ©é˜µ K
    K = np.exp(-M / reg)
    # print(f"Kernel çŸ©é˜µ K (éƒ¨åˆ†) (åŸºäº exp(-M/reg={reg})):\n", K[:min(3,n), :min(3,m)])

    # 4. Sinkhorn è¿­ä»£
    # print(f"\nğŸ”„ å¼€å§‹è¿­ä»£ (æœ€å¤š {num_iters} æ¬¡, æ”¶æ•›å®¹å¿åº¦ {tol:.1e})...")
    u = np.ones(n) / n 
    v = np.ones(m) / m 

    for i in range(num_iters):
        u_prev = u.copy()

        # æ›´æ–° v: v = b / (K^T u)
        KTu = K.T @ u
        v = b / (KTu + 1e-10) # åŠ ä¸€ä¸ªå°å¸¸æ•°é¿å…é™¤ä»¥é›¶

        # æ›´æ–° u: u = a / (K v)
        Kv = K @ v
        u = a / (Kv + 1e-10) # åŠ ä¸€ä¸ªå°å¸¸æ•°é¿å…é™¤ä»¥é›¶

        if verbose and (i % (num_iters // 10 if num_iters > 10 else 1) == 0 or i == num_iters - 1):
            # æ£€æŸ¥è¾¹é™…çº¦æŸçš„æ»¡è¶³ç¨‹åº¦ (å¯é€‰ï¼Œä½†æœ‰åŠ©äºè°ƒè¯•)
            # P_curr = u[:, np.newaxis] * K * v[np.newaxis, :]
            # err_a = np.linalg.norm(P_curr.sum(axis=1) - a)
            # err_b = np.linalg.norm(P_curr.sum(axis=0) - b)
            # print(f"è¿­ä»£ {i+1:03d}: uå˜åŒ– {np.linalg.norm(u - u_prev):.2e}, è¾¹é™…è¯¯å·® a: {err_a:.2e}, b: {err_b:.2e}")
            print(f"è¿­ä»£ {i+1:03d}: uå˜åŒ– {np.linalg.norm(u - u_prev):.2e}")

        if np.linalg.norm(u - u_prev) < tol:
            # print(f"âœ… åœ¨ç¬¬ {i+1} æ¬¡è¿­ä»£æ”¶æ•›ã€‚")
            break
        # if i == num_iters - 1: 
        #     print(f"âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {num_iters}ï¼Œå¯èƒ½æœªå®Œå…¨æ”¶æ•›ã€‚æœ€åçš„uå˜åŒ–: {np.linalg.norm(u - u_prev):.2e}")

    # 5. è®¡ç®—æœ€ä¼˜ä¼ è¾“è®¡åˆ’ P
    P = u[:, np.newaxis] * K * v[np.newaxis, :]
    # print("\nè¿‘ä¼¼çš„æœ€ä¼˜ä¼ è¾“è®¡åˆ’ P (éƒ¨åˆ†):\n", P[:min(3,n), :min(3,m)])

    # 6. è®¡ç®— Sinkhorn è·ç¦»
    sinkhorn_dist = np.sum(P * M)
    # print(f"\nğŸ† è®¡ç®—å¾—åˆ°çš„ Sinkhorn è·ç¦»: {sinkhorn_dist:.6f}")
    
    # éªŒè¯ P çš„è¾¹é™…
    # print(f"P çš„è¡Œå’Œ (åº”æ¥è¿‘ a): {P.sum(axis=1)[:min(5,n)]} ... (a: {a[:min(5,n)]} ...)")
    # print(f"P çš„åˆ—å’Œ (åº”æ¥è¿‘ b): {P.sum(axis=0)[:min(5,m)]} ... (b: {b[:min(5,m)]} ...)")
    # print(f"æ€»å’Œ P.sum() (åº”æ¥è¿‘ 1): {P.sum():.6f}")
    sinkhorn_dist = np.round(sinkhorn_dist, 6)
    return sinkhorn_dist

def kde_kl_divergence(x, y, xbins=1000, epsilon=1e-10):
    x = np.atleast_1d(x)
    y = np.atleast_1d(y)
    
    if len(x) == 0 or len(y) == 0:
        return np.inf
    # è®¡ç®— KDE
    kde_x = gaussian_kde(x)
    kde_y = gaussian_kde(y)
    
    # å®šä¹‰ç§¯åˆ†èŒƒå›´
    xmin = min(np.min(x), np.min(y))
    xmax = max(np.max(x), np.max(y))
    x_range = np.linspace(xmin, xmax, xbins)
    
    # è®¡ç®— KDE çš„æ¦‚ç‡å¯†åº¦
    dens_x = kde_x(x_range)
    dens_y = kde_y(x_range)
    dens_x = np.maximum(dens_x, epsilon)
    dens_y = np.maximum(dens_y, epsilon)
    # è®¡ç®— KL æ•£åº¦
    delta_x = np.abs(x_range[1] - x_range[0])
    kl_div = np.sum(dens_x * np.log(dens_x / dens_y)) * delta_x
    
    return kl_div

def kl_divergence_multivariate(x, y, xbins=1000, noise_level=1e-6):
    x = np.atleast_2d(x)
    y = np.atleast_2d(y)
    x = x+np.random.normal(0, noise_level, x.shape)
    y = y+np.random.normal(0, noise_level, y.shape)
    
    n_features = x.shape[1]
    
    # åˆå§‹åŒ– KL æ•£åº¦
    kl_div = 0.0
    
    for i in range(n_features):
        kl_div += kde_kl_divergence(x[:, i], y[:, i], xbins=xbins)
    
    return kl_div

def KL_loss(p, q):
    # æ£€æŸ¥è¾“å…¥çŸ©é˜µç»´åº¦
    loss = kl_divergence_multivariate(p, q)
    loss = np.round(loss, 4)
    return loss

def KL_loss_intergroup(p, q, idx):
    loss = [w[idx][j]*KL_loss(p,q) for j in range(w.shape[0])]
    return loss

def KL_loss_all(p, q):
    loss = KL_loss(p,q)
    return loss


def gaussian_kde_pdf(samples, eval_points, bandwidth):
    """
    é«˜æ–¯æ ¸å¯†åº¦ä¼°è®¡ PDF è®¡ç®— (æ”¯æŒé«˜ç»´)
    samples: (N, D)
    eval_points: (M, D)
    bandwidth: float
    return: (M,) PDF å€¼
    """
    N, D = samples.shape
    diff = eval_points.unsqueeze(1) - samples.unsqueeze(0)  # (M, N, D)
    # é«˜æ–¯æ ¸å…¬å¼
    kernel_vals = torch.exp(-0.5 * diff.pow(2).sum(dim=2) / (bandwidth ** 2))
    const = 1.0 / ((2 * torch.pi) ** (D / 2) * (bandwidth ** D))
    pdf_vals = const * kernel_vals.mean(dim=1)
    return pdf_vals

def kl_divergence_kde(p_samples, q_samples, bandwidth=None):
    """
    é«˜ç»´KDEä¼°è®¡KLæ•£åº¦
    p_samples: (Np, D)
    q_samples: (Nq, D)
    bandwidth: floatï¼Œå¯é€‰ã€‚å¦‚æœNoneåˆ™è‡ªåŠ¨é€‰å–
    return: KL(P||Q)
    """
    device = p_samples.device
    Np, D = p_samples.shape
    
    # è‡ªåŠ¨å¸¦å®½é€‰æ‹©ï¼ˆSilverman's ruleï¼‰
    if bandwidth is None:
        std_p = p_samples.std(dim=0).mean()
        std_q = q_samples.std(dim=0).mean()
        avg_std = (std_p + std_q) / 2
        bandwidth = 1.06 * avg_std * (Np ** (-1 / (D + 4)))
    
    # ç”¨p_samplesä½œä¸ºè¯„ä¼°ç‚¹
    eval_points = p_samples
    
    # KDEè®¡ç®—pdf
    p_pdf = gaussian_kde_pdf(p_samples, eval_points, bandwidth)
    q_pdf = gaussian_kde_pdf(q_samples, eval_points, bandwidth)
    
    eps = 1e-12
    kl_vals = torch.log((p_pdf + eps) / (q_pdf + eps))
    return kl_vals.mean().item()
def Jensen_loss(p, q):
    loss = 0.5 * KL_loss(p, q) + 0.5 * KL_loss(p, q)
    return loss
def loss_hinge_dis(dis_fake, dis_real):
    # loss_dist = torch.mean(dis_real)
    # loss_dist += torch.mean(-dis_fake)
    loss_dist = torch.mean(F.relu(1. + dis_real))
    loss_dist += torch.mean(F.relu(1. - dis_fake))
    return loss_dist

def loss_domain(pred_real, y_real):
    return F.cross_entropy(pred_real, y_real.squeeze())
def loss_hinge_gen(dis_fake):
    # loss = torch.mean(-dis_fake)
    loss = torch.mean(F.relu(1 - dis_fake))
    return loss

class AddNoise:
    def __init__(self, min=0.0, max=0.1):
        self.min = min
        self.max = max
        
    def __call__(self, tensor):
        noise = (torch.rand(tensor.shape)*(self.max-self.min)).to(tensor.device)
        return torch.clamp(tensor + noise, 0., 1.)
    
class LayerMonitor:
    def __init__(self, model, writer, log_interval=1):
        self.writer = writer
        self.log_interval = log_interval
        self.step = 0
        
        # è‡ªåŠ¨æ³¨å†Œæ‰€æœ‰å±‚çš„hook
        self.handles = []
        for name, layer in model.named_modules():
            if not isinstance(layer, (nn.ModuleList, nn.Sequential)):
                handle = layer.register_forward_hook(
                    self._create_hook(name)
                )
                self.handles.append(handle)
    
    def _create_hook(self, layer_name):
        def hook(module, input, output):
            # æ¯éš”æŒ‡å®šæ­¥æ•°è®°å½•
            if len(output) == 2:
                return output
            if self.step % self.log_interval == 0:
                with torch.no_grad():
                    # æ ‡é‡ç»Ÿè®¡
                    self.writer.add_scalar(
                        f"activations/{layer_name}/mean", 
                        output.mean(), self.step
                    )
                    self.writer.add_scalar(
                        f"activations/{layer_name}/std",
                        output.std(), self.step
                    )
                    
                    # ç›´æ–¹å›¾
                    self.writer.add_histogram(
                        f"activations_hist/{layer_name}",
                        output.cpu(),
                        self.step
                    )
                    
                    # æ¢¯åº¦ç»Ÿè®¡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if output.requires_grad:
                        output.register_hook(
                            lambda grad: self._grad_hook(grad, layer_name)
                        )
            return output
        return hook
    
    def _grad_hook(self, grad, layer_name):
        self.writer.add_scalar(
            f"gradients/{layer_name}/mean",
            grad.mean(), self.step
        )
        self.writer.add_histogram(
            f"gradients_hist/{layer_name}",
            grad.cpu(),
            self.step
        )
    
    def update_step(self):
        self.step += 1
        
    def remove(self):
        """ç§»é™¤æ‰€æœ‰hook"""
        for handle in self.handles:
            handle.remove()